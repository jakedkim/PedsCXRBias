{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training 25 aggregate models \n",
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# File paths and setup\n",
    "parent = '/home/jkim/research/peds_cxr/'\n",
    "# Predefined 25 random seeds\n",
    "predefined_seeds = [2358, 4563, 7894, 13289, 15892, 19756, 23890, 27456, 30890, 34905, 38764, 42136, 46578, 50349, 54892, 59871, 63912, 68354, 72109, 76845, 80492, 84910, 89357, 93450, 97910]\n",
    "\n",
    "### set source\n",
    "metadata_source = parent + 'no_finding/metadata/aggregate_age_nf/' \n",
    "\n",
    "for seed in predefined_seeds:\n",
    "    for five in ['0', '25', '50', '75', '100']:\n",
    "        image_source = parent + f'no_finding/images/age/{seed}/age_{five}%_seed{seed}'\n",
    "        # Load the data from the CSV file into a pandas DataFrame\n",
    "        aggregate_metadata_path = metadata_source + f'aggregate_age_nf_seed_{seed}/age_{five}_seed_{seed}.csv'\n",
    "        aggregate_metadata = pd.read_csv(aggregate_metadata_path)\n",
    "\n",
    "        # Creating image_paths.txt\n",
    "        # Filter rows where 'Set' column is not \"not assigned\"\n",
    "        filtered_data = aggregate_metadata[aggregate_metadata['Set'] != \"not assigned\"]\n",
    "        image_path = filtered_data['Image Index']\n",
    "        output_image_txt = image_source + '/image_paths.txt'\n",
    "        image_path.to_csv(output_image_txt, sep=',', index=False, header=False)\n",
    "\n",
    "        # Creating train.txt \n",
    "        train_index = aggregate_metadata[aggregate_metadata['Set']=='train']\n",
    "        aggregate_train_output = image_source + '/train.txt'\n",
    "        aggregate_train_txt = train_index['Image Index'] \n",
    "        aggregate_train_txt.to_csv(aggregate_train_output, sep=',', index=False, header=False)\n",
    "\n",
    "        # Creating val.txt \n",
    "        val_index = aggregate_metadata[aggregate_metadata['Set']=='val']\n",
    "        aggregate_val_output = image_source + '/val.txt'\n",
    "        aggregate_val_txt = val_index['Image Index'] \n",
    "        aggregate_val_txt.to_csv(aggregate_val_output, sep=',', index=False, header=False)\n",
    "\n",
    "        # Creating test.txt\n",
    "        test_index = aggregate_metadata[aggregate_metadata['Set']=='test']\n",
    "        aggregate_test_output = image_source + '/aggregate_test.txt'\n",
    "        aggregate_test_txt = test_index['Image Index'] \n",
    "        aggregate_test_txt.to_csv(aggregate_test_output, sep=',', index=False, header=False)\n",
    "\n",
    "        # Creating labels.csv\n",
    "        aggregate_labels_output = image_source + '/labels.csv'\n",
    "        aggregate_labels = filtered_data[['Image Index', 'No Finding', 'Cardiomegaly', 'Consolidation', 'Infiltration', 'Mass/Nodule', 'Pneumonia']]\n",
    "        # rename Image Index to Image\n",
    "        aggregate_labels = aggregate_labels.rename(columns={'Image Index': 'Image'})\n",
    "        aggregate_labels.to_csv(aggregate_labels_output, sep=',', index=False)\n",
    "\n",
    "        # confirming the number of train val and test. \n",
    "        print(f'age, seed: {seed}, {five}%')\n",
    "        print(\"# of train:\",len(aggregate_train_txt))\n",
    "        print('# of val:',len(aggregate_val_txt))\n",
    "        print('# of test:',len(aggregate_test_txt))\n",
    "        print('# of image path:',len(image_path))\n",
    "        print('# of labels:',len(aggregate_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create json file. \n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Loop through each seed to generate JSON files\n",
    "for seed in predefined_seeds:\n",
    "    for five in ['0', '25', '50', '75', '100']:\n",
    "        json_template = {\n",
    "        \"data_dir\": f\"/home/jkim/research/peds_cxr/no_finding/images/age/{seed}/\",\n",
    "        \"labels\": ['No Finding', 'Cardiomegaly', 'Consolidation', 'Infiltration', 'Mass/Nodule' , 'Pneumonia'],\n",
    "        \"dataset_nicenames\": {\n",
    "            \"nihcxr14_test\": \"NIH CXR14\",\n",
    "            \"chexpert_test\": \"CheXpert\",\n",
    "            \"mimic_test\": \"MIMIC\",\n",
    "            \"padchest_test\": \"PadChest\",\n",
    "            \"rsna_pneumonia_test_100\": \"RSNA Pneumonia\",\n",
    "            \"rsna_pneumonia_DeiT-B_test_100\": \"RSNA Pneumonia DeiT-B\",\n",
    "            \"Aggregate_DenseNet121_aggregate_test\": \"aggregate DenseNet121\",\n",
    "            \"Aggregate_DenseNet121_aggregate_test\": \"Aggregate DenseNet121\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Construct the new directory path\n",
    "        new_data_dir = f\"/home/jkim/research/peds_cxr/no_finding/images/age/{seed}/age_{five}%_seed{seed}/\"\n",
    "        \n",
    "        # Create the new directory if it doesn't exist\n",
    "        if not os.path.exists(new_data_dir):\n",
    "            print(f\"The directory {new_data_dir} does not exist. Creating it.\")\n",
    "            os.makedirs(new_data_dir)\n",
    "\n",
    "        # Specify the path for the new JSON file\n",
    "        new_json_path = os.path.join(new_data_dir, f\"cfg_aggregate_{seed}.json\")\n",
    "\n",
    "        # Write the JSON file\n",
    "        with open(new_json_path, 'w') as f:\n",
    "            json.dump(json_template, f, indent=4)\n",
    "        \n",
    "        print(f\"Created new JSON file at {new_json_path}\")\n",
    "\n",
    "# batch creating result directiory\n",
    "for seed in predefined_seeds:\n",
    "    for five in ['0', '25', '50', '75', '100']:\n",
    "        # Construct the new directory path for results\n",
    "        new_result_dir = f\"/home/jkim/research/peds_cxr/no_finding/results/age/train/{seed}/age_{five}%_seed{seed}/\"\n",
    "        \n",
    "        # Create the new directory if it doesn't exist\n",
    "        if not os.path.exists(new_result_dir):\n",
    "            os.makedirs(new_result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#training 25 aggregate models\n",
    "optimizers = ['SGD']\n",
    "initial_lrs = [0.01]\n",
    "weight_decays = [1e-5]\n",
    "dropouts = [0.5]\n",
    "gpu='0' \n",
    "\n",
    "parent = '/home/jkim/research/peds_cxr/'\n",
    "import subprocess\n",
    "    \n",
    "# Iterate over the hyperparameters\n",
    "for seed in predefined_seeds:\n",
    "    for five in ['0', '25', '50', '75', '100']:    \n",
    "        for optimizer in optimizers:\n",
    "            for initial_lr in initial_lrs:\n",
    "                for weight_decay in weight_decays:\n",
    "                    for dropout in dropouts:\n",
    "                        # Construct the command with updated hyperparameters\n",
    "                        command = [\n",
    "                            'python3',\n",
    "                            parent + 'transformer-radiographs/train_cxr.py',\n",
    "                            '--cfg-dir',\n",
    "                            parent + f'no_finding/images/age/{seed}/age_{five}%_seed{seed}/cfg_aggregate_{seed}.json',\n",
    "                            '--dataset',\n",
    "                            f'age_{five}%_seed{seed}',\n",
    "                            '--labels-set',\n",
    "                            'labels',\n",
    "                            '--architecture',\n",
    "                            'DenseNet121',\n",
    "                            '--results-dir',\n",
    "                            parent + f'no_finding/results/age/train/{seed}/age_{five}%_seed{seed}/',\n",
    "                            '--optimizer-family',\n",
    "                            optimizer,\n",
    "                            '--dropout',\n",
    "                            str(dropout),\n",
    "                            '--weight-decay',\n",
    "                            str(weight_decay),\n",
    "                            '--initial-lr',\n",
    "                            str(initial_lr),\n",
    "                            '--drop-factor',\n",
    "                            '0.1',\n",
    "                            '--plateau-patience',\n",
    "                            '3',\n",
    "                            '--plateau-threshold',\n",
    "                            '1e-4',\n",
    "                            '--break-patience',\n",
    "                            '5',\n",
    "                            '--train-transform',\n",
    "                            'peds',\n",
    "                            '--train-file',\n",
    "                            'train.txt',\n",
    "                            '--val-file',\n",
    "                            'val.txt',\n",
    "                            '--use-gpus',\n",
    "                            gpu\n",
    "                        ]\n",
    "\n",
    "                        # Execute the command\n",
    "                        subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predefined_seeds = [2358, 4563, 7894, 13289, 15892, 19756, 23890, 27456, 30890, 34905, 38764, 42136, 46578, 50349, 54892, 59871, 63912, 68354, 72109, 76845, 80492, 84910, 89357, 93450, 97910]\n",
    "\n",
    "\n",
    "# testing 25 aggregate models - DenseNet121_lr0.01_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain.txt_vaval.txt_tfpeds_nlbatch_do0.5_1705144996.txt\n",
    "import subprocess\n",
    "import pandas as pd \n",
    "import os\n",
    "import glob\n",
    "# batch creating result directiory\n",
    "for seed in predefined_seeds:\n",
    "    for dir in ['all', 'young', 'old']:  \n",
    "        for five in ['0', '25', '50', '75', '100']:\n",
    "            # Construct the new directory path for results\n",
    "            new_result_dir = parent + f\"no_finding/results/age/test/{dir}/{seed}/age_{five}_{seed}_{dir}/\"\n",
    "\n",
    "            # Create the new directory if it doesn't exist\n",
    "            if not os.path.exists(new_result_dir):\n",
    "                os.makedirs(new_result_dir)\n",
    "\n",
    "for seed in predefined_seeds:\n",
    "    for five in ['0', '25', '50', '75', '100']:\n",
    "        aggregate_metadata_path = parent + f'no_finding/metadata/aggregate_age_nf/aggregate_age_nf_seed_{seed}/age_{five}_seed_{seed}.csv'\n",
    "        aggregate_metadata = pd.read_csv(aggregate_metadata_path)\n",
    "\n",
    "        # Creating aggregate_test_young / old .txt\n",
    "        ages = ['young', 'old']\n",
    "        for age in ages:\n",
    "            test_index = aggregate_metadata[(aggregate_metadata['Set'] == 'test') & (aggregate_metadata['Age Group'] == age)]\n",
    "            image_source = parent + f'no_finding/images/age/{seed}/age_{five}%_seed{seed}/'\n",
    "            aggregate_test_output = image_source + 'aggregate_test_'+ age +'.txt'\n",
    "            aggregate_test_txt = test_index['Image Index']\n",
    "            aggregate_test_txt.to_csv(aggregate_test_output, sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing 25 aggregate models - DenseNet121_lr0.01_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain.txt_vaval.txt_tfpeds_nlbatch_do0.5_1705144996.txt\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd \n",
    "import glob\n",
    "\n",
    "\n",
    "ages = ['old', 'young']\n",
    "# Testing old vs young\n",
    "for age in ages:\n",
    "    for seed in predefined_seeds:\n",
    "        for five in ['0', '25', '50', '75', '100']:\n",
    "        # Build the pattern to search for model files\n",
    "            search_pattern = os.path.join(parent, f\"no_finding/results/age/train/{seed}/age_{five}%_seed{seed}/DenseNet121_lr0.01_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain.txt_vaval.txt_tfpeds_nlbatch_do0.5_*_model.pt\")\n",
    "            \n",
    "            # Use glob to find the file\n",
    "            matching_files = glob.glob(search_pattern)\n",
    "            \n",
    "            # Check if any matching files are found\n",
    "            if not matching_files:\n",
    "                print(f\"No matching model files found for seed {seed}.\")\n",
    "                continue  # Skip to the next iteration\n",
    "\n",
    "            # Assuming there's only one matching file, take the first one\n",
    "            model_file_path = matching_files[0]\n",
    "\n",
    "            image_source = parent + f\"no_finding/images/age/{seed}/age_{five}%_seed{seed}/\"\n",
    "            command = [\n",
    "                \"python3\",\n",
    "                parent + \"transformer-radiographs/test_cxr.py\",\n",
    "                \"--cfg-dir\", image_source + f\"cfg_aggregate_{seed}.json\",\n",
    "                \"--dataset\", f'age_{five}%_seed{seed}',\n",
    "                \"--labels-set\", \"labels\",\n",
    "                \"--model-state\", model_file_path,\n",
    "                \"--model-type\", \"DenseNet121\",\n",
    "                \"--results-dir\", parent + f\"no_finding/results/age/test/{age}/{seed}/age_{five}_{seed}_{age}/\",\n",
    "                \"--test-file\", image_source + \"aggregate_test_\" + age + \".txt\",\n",
    "                \"--use-gpus\", gpu\n",
    "        ]\n",
    "\n",
    "            # Run the command\n",
    "            try:\n",
    "                subprocess.run(command, check=True)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"An error occurred while executing the command: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os \n",
    "import glob\n",
    "\n",
    "# setting directories\n",
    "parent = '/home/jkim/research/peds_cxr/'\n",
    "\n",
    "ages = ['old','young']\n",
    "age_file_name = ['_old', '_young']\n",
    "for age, age_name in zip(ages, age_file_name):\n",
    "    for seed in predefined_seeds:\n",
    "        for five in ['0', '25', '50', '75', '100']:       \n",
    "            # Build the pattern to search for model files\n",
    "                search_pattern = os.path.join(parent, f\"no_finding/results/age/test/{age}/{seed}/age_{five}_{seed}_{age}/DenseNet121_lr0.01_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain.txt_vaval.txt_tfpeds_nlbatch_do0.5_*.pkl\")\n",
    "\n",
    "                # Use glob to find the file\n",
    "                matching_files = glob.glob(search_pattern)\n",
    "                \n",
    "                # Check if any matching files are found\n",
    "                if not matching_files:\n",
    "                    print(f\"No matching model files found for seed {seed}.\")\n",
    "                    continue  # Skip to the next iteration\n",
    "\n",
    "                pkl_directory = matching_files[0]\n",
    "\n",
    "                directory = parent + f'no_finding/results/age/unpickle/{age}/'\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "\n",
    "                # Open the pickle file and load the data\n",
    "                with open(pkl_directory, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "\n",
    "                # Extract the nested dictionary\n",
    "                nested_dict = data.get(f'age_{five}%_seed{seed}_DenseNet121_aggregate_test_{age}', {})\n",
    "\n",
    "                # Create a DataFrame from the nested dictionary with keys 'y', 'yhat', and 'file'\n",
    "                df_nested = pd.DataFrame({\n",
    "                    \"y\": nested_dict[\"y\"],\n",
    "                    \"yhat\": nested_dict[\"yhat\"],\n",
    "                    \"file\": nested_dict[\"file\"]\n",
    "                })\n",
    "\n",
    "                # Define the file path\n",
    "                nested_file_path = directory + f'aggregate_{seed}_{age}_{five}_pkl.csv'\n",
    "\n",
    "                # Save the DataFrame to a CSV file\n",
    "                df_nested.to_csv(nested_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Pkl file as csv file \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
    "import ast\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Changing pickle file from dictionary format to list format \n",
    "ages = ['old', 'young']\n",
    "\n",
    "# create AUC directory in results\n",
    "if not os.path.exists(parent + 'no_finding/results/age/analysis/'):\n",
    "    os.makedirs(parent + 'no_finding/results/age/analysis/')\n",
    "if not os.path.exists(parent + 'no_finding/results/age/analysis/plot/age'):\n",
    "    os.makedirs(parent + 'no_finding/results/age/analysis/plot/age')\n",
    "if not os.path.exists(parent + 'no_finding/results/age/analysis/csv'):\n",
    "    os.makedirs(parent + 'no_finding/results/age/analysis/csv')\n",
    "\n",
    "for age in ages: \n",
    "    os.makedirs(parent + f'no_finding/results/age/analysis/unpickledcsv/{age}', exist_ok=True)  \n",
    "\n",
    "for seed in predefined_seeds:        \n",
    "    for age in ages: \n",
    "        for five in ['0', '25', '50', '75', '100']:\n",
    "            # set directories\n",
    "            pkl_dir = parent + f'no_finding/results/age/unpickle/{age}/aggregate_{seed}_{age}_{five}_pkl.csv'\n",
    "\n",
    "            # Loading the pkl file\n",
    "            aggregate_pkl = pd.read_csv(pkl_dir)\n",
    "\n",
    "            # Defining the aggregate_label as the list of labels\n",
    "            aggregate_label = [\"No Finding\", \"Cardiomegaly\", \"Consolidation\", \"Infiltration\", \"Mass/Nodule\", \"Pneumonia\"]\n",
    "\n",
    "            # Converting the string representation of lists to actual lists\n",
    "            aggregate_pkl['y'] = aggregate_pkl['y'].apply(ast.literal_eval)\n",
    "            aggregate_pkl['yhat'] = aggregate_pkl['yhat'].apply(ast.literal_eval)\n",
    "\n",
    "            # Splitting the one-hot encoded labels and predictions into separate columns\n",
    "            y_true_df = pd.DataFrame(aggregate_pkl['y'].tolist(), columns=aggregate_label)\n",
    "            y_pred_df = pd.DataFrame(aggregate_pkl['yhat'].tolist(), columns=aggregate_label)\n",
    "\n",
    "            # Merging the true labels and predicted probabilities based on index\n",
    "            result_df = pd.concat([y_true_df, y_pred_df.add_suffix('_pred')], axis=1)\n",
    "\n",
    "            # Saving\n",
    "            result_df.to_csv(parent + f'no_finding/results/age/analysis/unpickledcsv/{age}/aggregate_{seed}_{age}_{five}_pkl.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jkim/research/peds_cxr/no_finding/results/age_young/analysis/unpickledcsv/young/aggregate_2358_young_0_pkl.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subgroup \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoung\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m predefined_seeds:\n\u001b[1;32m     57\u001b[0m         dfs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 58\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoung_0\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/young/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_young_0_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoung_25\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/young/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_young_25_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoung_50\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/young/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_young_50_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoung_75\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/young/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_young_75_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoung_100\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/young/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_young_100_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_0\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/old/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_old_0_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_25\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/old/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_old_25_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_50\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/old/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_old_50_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_75\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/old/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_old_75_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_100\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(parent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_finding/results/age_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/analysis/unpickledcsv/old/aggregate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_old_100_pkl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     68\u001b[0m         }\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, df \u001b[38;5;129;01min\u001b[39;00m dfs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     71\u001b[0m             label_metrics \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/peds/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/peds/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/peds/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.conda/envs/peds/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/peds/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/.conda/envs/peds/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/peds/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jkim/research/peds_cxr/no_finding/results/age_young/analysis/unpickledcsv/young/aggregate_2358_young_0_pkl.csv'"
     ]
    }
   ],
   "source": [
    "# stats and plotting for all aggregate data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "\n",
    "parent = '/home/jkim/research/peds_cxr/'\n",
    "predefined_seeds = [2358, 4563, 7894, 13289, 15892, 19756, 23890, 27456, 30890, 34905, 38764, 42136, 46578, 50349, 54892, 59871, 63912, 68354, 72109, 76845, 80492, 84910, 89357, 93450, 97910]\n",
    "\n",
    "# Initialize variables\n",
    "labels = [\"No Finding\", \"Cardiomegaly\", \"Consolidation\", \"Infiltration\", \"Mass/Nodule\", \"Pneumonia\"]\n",
    "\n",
    "def calculate_metrics_and_auroc(df, label):\n",
    "    true_labels = df[label]\n",
    "    predicted_scores = df[f\"{label}_pred\"]\n",
    "    \n",
    "    # Calculate AUROC\n",
    "    auroc = roc_auc_score(true_labels, predicted_scores)\n",
    "\n",
    "    # Calculate optimal threshold using Youden's Index\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predicted_scores)\n",
    "    youden_index = tpr - fpr\n",
    "    optimal_idx = np.argmax(youden_index)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Other metrics\n",
    "    predicted_labels = (predicted_scores >= optimal_threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "    npv = tn / (tn + fn) if tn + fn != 0 else 0\n",
    "    f1_score = 2 * (ppv * sensitivity) / (ppv + sensitivity) if ppv + sensitivity != 0 else 0\n",
    "    fpr = fp / (fp + tn)  \n",
    "    fnr = fn / (fn + tp)\n",
    "    \n",
    "    return [auroc, sensitivity, specificity, ppv, npv, f1_score, fpr, fnr, optimal_threshold]\n",
    "\n",
    "# Initialize empty DataFrames for storing metrics\n",
    "cols = ['Seed', 'Label', 'AUROC', 'Sensitivity', 'Specificity', 'PPV', 'NPV', 'F1 Score', 'FPR', 'FNR', 'Youden_Threshold'] \n",
    "metrics_dfs = {\n",
    "    \"young_0\": pd.DataFrame(columns=cols),\n",
    "    \"young_25\": pd.DataFrame(columns=cols),\n",
    "    \"young_50\": pd.DataFrame(columns=cols),\n",
    "    \"young_75\": pd.DataFrame(columns=cols),\n",
    "    \"young_100\": pd.DataFrame(columns=cols),\n",
    "    \"old_0\": pd.DataFrame(columns=cols),\n",
    "    \"old_25\": pd.DataFrame(columns=cols),\n",
    "    \"old_50\": pd.DataFrame(columns=cols),\n",
    "    \"old_75\": pd.DataFrame(columns=cols),\n",
    "    \"old_100\": pd.DataFrame(columns=cols),\n",
    "}\n",
    "\n",
    "# Loop through subgroups ('young', 'old') and seeds\n",
    "for subgroup in [\"young\", \"old\"]:\n",
    "    for seed in predefined_seeds:\n",
    "        dfs = {\n",
    "            \"young_0\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/young/aggregate_{seed}_young_0_pkl.csv\"),\n",
    "            \"young_25\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/young/aggregate_{seed}_young_25_pkl.csv\"),\n",
    "            \"young_50\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/young/aggregate_{seed}_young_50_pkl.csv\"),\n",
    "            \"young_75\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/young/aggregate_{seed}_young_75_pkl.csv\"),\n",
    "            \"young_100\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/young/aggregate_{seed}_young_100_pkl.csv\"),\n",
    "            \"old_0\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/old/aggregate_{seed}_old_0_pkl.csv\"),\n",
    "            \"old_25\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/old/aggregate_{seed}_old_25_pkl.csv\"),\n",
    "            \"old_50\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/old/aggregate_{seed}_old_50_pkl.csv\"),\n",
    "            \"old_75\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/old/aggregate_{seed}_old_75_pkl.csv\"),\n",
    "            \"old_100\": pd.read_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/unpickledcsv/old/aggregate_{seed}_old_100_pkl.csv\"),\n",
    "        }\n",
    "\n",
    "        for key, df in dfs.items():\n",
    "            label_metrics = []\n",
    "            for label in labels:\n",
    "                metrics = calculate_metrics_and_auroc(df, label)\n",
    "                label_metrics.append(metrics)\n",
    "                metrics_dfs[key] = metrics_dfs[key].append(pd.Series([seed, label] + metrics, index=cols), ignore_index=True)\n",
    "\n",
    "            # Calculate and append the \"All Label\" metrics\n",
    "            all_label_metrics = np.mean(label_metrics, axis=0).tolist()\n",
    "            metrics_dfs[key] = metrics_dfs[key].append(pd.Series([seed, \"All Label\"] + all_label_metrics, index=cols), ignore_index=True)\n",
    "\n",
    "    # Save the metrics DataFrames to CSV for each subgroup\n",
    "    for key, df in metrics_dfs.items():\n",
    "        df.to_csv(parent + f\"no_finding/results/age_{subgroup}/analysis/csv/metrics_{key}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Define paths and labels\n",
    "parent = '/home/jkim/research/peds_cxr/'\n",
    "labels = [\"All Label\", \"No Finding\", \"Cardiomegaly\", \"Consolidation\", \"Infiltration\", \"Mass/Nodule\", \"Pneumonia\"]\n",
    "metrics_to_analyze = ['AUROC', 'Sensitivity', 'Specificity', 'PPV', 'NPV', 'F1 Score', 'FPR', 'FNR']\n",
    "ratios = ['0', '25', '50', '75', '100']\n",
    "\n",
    "# File paths for Young and Old datasets\n",
    "young_files = [f\"{parent}no_finding/results/age/analysis/csv/metrics_young_{ratio}.csv\" for ratio in ratios]\n",
    "old_files = [f\"{parent}no_finding/results/age/analysis/csv/metrics_old_{ratio}.csv\" for ratio in ratios]\n",
    "\n",
    "# Define function to perform comparisons and generate combined stats\n",
    "def analyze_and_combine(young_files, old_files, ratios):\n",
    "    stats_df = pd.DataFrame(columns=['Metric', 'Label', 'Comparison', 'p_value', 'p_value_corrected'])\n",
    "    \n",
    "    # Perform Young vs Old comparisons\n",
    "    for ratio, y_file, o_file in zip(ratios, young_files, old_files):\n",
    "        df_metrics_young = pd.read_csv(y_file)\n",
    "        df_metrics_old = pd.read_csv(o_file)\n",
    "        \n",
    "        for metric in metrics_to_analyze:\n",
    "            for label in labels:\n",
    "                metric_values_young = df_metrics_young[df_metrics_young['Label'] == label][metric].dropna()\n",
    "                metric_values_old = df_metrics_old[df_metrics_old['Label'] == label][metric].dropna()\n",
    "                \n",
    "                if len(metric_values_young) > 0 and len(metric_values_old) > 0:\n",
    "                    _, p_value = ttest_rel(metric_values_young, metric_values_old)\n",
    "                    stats_df = stats_df.append({\n",
    "                        'Metric': metric,\n",
    "                        'Label': label,\n",
    "                        'Comparison': f'young_{ratio} vs old_{ratio}',\n",
    "                        'p_value': p_value\n",
    "                    }, ignore_index=True)\n",
    "    \n",
    "    # Perform cross-ratio comparisons (e.g., young_0 vs old_100 and young_100 vs old_0)\n",
    "    cross_comparisons = [('0', '100'), ('100', '0')]\n",
    "    for y_ratio, o_ratio in cross_comparisons:\n",
    "        y_file = f\"{parent}no_finding/results/age/analysis/csv/metrics_young_{y_ratio}.csv\"\n",
    "        o_file = f\"{parent}no_finding/results/age/analysis/csv/metrics_old_{o_ratio}.csv\"\n",
    "        df_metrics_young = pd.read_csv(y_file)\n",
    "        df_metrics_old = pd.read_csv(o_file)\n",
    "\n",
    "        for metric in metrics_to_analyze:\n",
    "            for label in labels:\n",
    "                metric_values_young = df_metrics_young[df_metrics_young['Label'] == label][metric].dropna()\n",
    "                metric_values_old = df_metrics_old[df_metrics_old['Label'] == label][metric].dropna()\n",
    "\n",
    "                if len(metric_values_young) > 0 and len(metric_values_old) > 0:\n",
    "                    _, p_value = ttest_rel(metric_values_young, metric_values_old)\n",
    "                    stats_df = stats_df.append({\n",
    "                        'Metric': metric,\n",
    "                        'Label': label,\n",
    "                        'Comparison': f'young_{y_ratio} vs old_{o_ratio}',\n",
    "                        'p_value': p_value\n",
    "                    }, ignore_index=True)\n",
    "\n",
    "    # Perform within-group comparisons (e.g., Old_0 vs Old_50)\n",
    "    def analyze_within_group(df_list, group_label):\n",
    "        local_stats = pd.DataFrame(columns=['Metric', 'Label', 'Comparison', 'p_value'])\n",
    "        \n",
    "        for metric in metrics_to_analyze:\n",
    "            for label in labels:\n",
    "                for i in range(len(df_list) - 1):\n",
    "                    for j in range(i + 1, len(df_list)):\n",
    "                        metric_values_i = df_list[i][df_list[i]['Label'] == label][metric].dropna()\n",
    "                        metric_values_j = df_list[j][df_list[j]['Label'] == label][metric].dropna()\n",
    "                        \n",
    "                        if len(metric_values_i) > 0 and len(metric_values_j) > 0:\n",
    "                            _, p_value = ttest_rel(metric_values_i, metric_values_j)\n",
    "                            comparison = f'{group_label}_{ratios[i]} vs {group_label}_{ratios[j]}'\n",
    "                            local_stats = local_stats.append({\n",
    "                                'Metric': metric,\n",
    "                                'Label': label,\n",
    "                                'Comparison': comparison,\n",
    "                                'p_value': p_value\n",
    "                            }, ignore_index=True)\n",
    "        \n",
    "        return local_stats\n",
    "\n",
    "    # Load datasets for within-group comparisons\n",
    "    young_datasets = [pd.read_csv(f) for f in young_files]\n",
    "    old_datasets = [pd.read_csv(f) for f in old_files]\n",
    "\n",
    "    stats_df_young = analyze_within_group(young_datasets, 'young')\n",
    "    stats_df_old = analyze_within_group(old_datasets, 'old')\n",
    "    \n",
    "    # Combine results\n",
    "    combined_stats = pd.concat([stats_df, stats_df_young, stats_df_old], ignore_index=True)\n",
    "    \n",
    "    # Apply Benjamini-Hochberg correction for multiple comparisons\n",
    "    p_values = combined_stats['p_value'].astype(float).to_numpy()\n",
    "    _, pvals_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "    combined_stats['p_value_corrected'] = pvals_corrected\n",
    "    \n",
    "    return combined_stats\n",
    "\n",
    "# Analyze and combine results\n",
    "final_stats_df = analyze_and_combine(young_files, old_files, ratios)\n",
    "\n",
    "# Save the final combined CSV\n",
    "final_stats_df.to_csv(parent + \"no_finding/results/age/analysis/csv/final_pvalues.csv\", index=False)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# Load CSV files and calculate statistics\n",
    "def process_datasets(age_prefix, ratios):\n",
    "    stats_df = pd.DataFrame(columns=['Age', 'ratio', 'Label', 'Metric', 'Mean', 'CI_Lower', 'CI_Upper'])\n",
    "    \n",
    "    for ratio in ratios:\n",
    "        df_metrics = pd.read_csv(parent + f\"no_finding/results/age/analysis/csv/metrics_{age_prefix}_{ratio}.csv\")\n",
    "        for label in labels:\n",
    "            for metric in metrics_to_analyze:\n",
    "                metric_values = df_metrics[df_metrics['Label'] == label][metric].dropna()\n",
    "                if not metric_values.empty:\n",
    "                    mean_value = metric_values.mean()\n",
    "                    stderr = metric_values.sem()\n",
    "                    ci = stderr * 1.96  # For 95% confidence interval\n",
    "                    ci_lower = mean_value - ci\n",
    "                    ci_upper = mean_value + ci\n",
    "                    stats_df = stats_df.append({\n",
    "                        'Age': age_prefix,\n",
    "                        'ratio': ratio,\n",
    "                        'Label': label,\n",
    "                        'Metric': metric,\n",
    "                        'Mean': mean_value,\n",
    "                        'CI_Lower': ci_lower,\n",
    "                        'CI_Upper': ci_upper\n",
    "                    }, ignore_index=True)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "# Process datasets for Young and Old\n",
    "stats_df_young = process_datasets('young', ratios)\n",
    "stats_df_old = process_datasets('old', ratios)\n",
    "\n",
    "# Save the summary DataFrames to CSV\n",
    "stats_df_young.to_csv(parent + \"no_finding/results/age/analysis/csv/aggregate_young_stats.csv\", index=False)\n",
    "stats_df_old.to_csv(parent + \"no_finding/results/age/analysis/csv/aggregate_old_stats.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define parent path\n",
    "parent = '/home/jkim/research/peds_cxr/'\n",
    "\n",
    "# Load datasets for the young and old age groups\n",
    "ratios = ['0', '25', '50', '75', '100']\n",
    "metrics_young = {ratio: pd.read_csv(parent + f'no_finding/results/age/analysis/csv/metrics_young_{ratio}.csv') for ratio in ratios}\n",
    "metrics_old = {ratio: pd.read_csv(parent + f'no_finding/results/age/analysis/csv/metrics_old_{ratio}.csv') for ratio in ratios}\n",
    "aggregate_young_vs_old_pvalue = pd.read_csv(parent + 'no_finding/results/age/analysis/csv/final_pvalues.csv')\n",
    "\n",
    "# Define unique metrics and labels\n",
    "unique_metrics = ['AUROC', 'Sensitivity', 'Specificity', 'FPR', 'FNR', 'PPV', 'NPV']\n",
    "unique_labels = metrics_young['0']['Label'].unique()\n",
    "neutral_palette = [\"#b5d1ae\", \"#1b485e\"]\n",
    "\n",
    "# Iterate over each ratio and metric to create plots comparing young and old groups\n",
    "for ratio in ratios:\n",
    "    # Add a \"Condition\" column and combine metrics for the current ratio\n",
    "    metrics_young[ratio]['Condition'] = 'Young'\n",
    "    metrics_old[ratio]['Condition'] = 'Old'\n",
    "    combined_metrics = pd.concat([metrics_young[ratio], metrics_old[ratio]])\n",
    "\n",
    "    for metric in unique_metrics:\n",
    "        # Create a single plot for each metric and ratio\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot the boxplots for young and old comparison\n",
    "        sns.boxplot(x='Label', y=metric, hue='Condition', data=combined_metrics, ax=ax, palette=neutral_palette, order=unique_labels, width=0.4)\n",
    "\n",
    "        # Calculate p-values for the current metric\n",
    "        p_values = []\n",
    "        for label in unique_labels:\n",
    "            filtered_df = aggregate_young_vs_old_pvalue[\n",
    "                (aggregate_young_vs_old_pvalue['Metric'] == metric) & \n",
    "                (aggregate_young_vs_old_pvalue['Label'] == label) & \n",
    "                (aggregate_young_vs_old_pvalue['Comparison'].str.contains(f'young_{ratio} vs old_{ratio}'))\n",
    "            ]\n",
    "            p_value = filtered_df['p_value_corrected'].values[0] if not filtered_df.empty else np.nan\n",
    "            p_values.append(p_value)\n",
    "\n",
    "        # Add statistical annotation for young vs old comparison\n",
    "        box_pairs = [((label, 'Young'), (label, 'Old')) for label in unique_labels]\n",
    "        add_stat_annotation(ax, data=combined_metrics, x='Label', y=metric, hue='Condition',\n",
    "                            box_pairs=box_pairs, perform_stat_test=False, pvalues=p_values, \n",
    "                            test_short_name='Custom', loc='inside', verbose=2, \n",
    "                            pvalue_thresholds=[(0.001, '***'), (0.01, '**'), (0.05, '*'), (1, 'ns')])\n",
    "        ax.set_title(f'{metric} Comparison Between Young and Old ({ratio}% Old in Training Set)', fontsize=20, fontweight='bold')\n",
    "        ax.set_xlabel('Labels', fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel(metric, fontsize=16, fontweight='bold')\n",
    "        ax.tick_params(axis='x', labelsize=14, rotation=45)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        ax.grid(True, linestyle='--', linewidth=0.5, color='gray', axis='y')\n",
    "\n",
    "        # Create a combined legend\n",
    "        handles, labels = ax.get_legend_handles_labels()  \n",
    "        new_labels = ['Young', 'Old']\n",
    "        ax.legend(handles, new_labels, title=\"Legend\", loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "        # Adjust layout and save\n",
    "        plt.tight_layout()\n",
    "        plot_file_path = parent + f'no_finding/results/age/analysis/plot/age/{metric}_young_{ratio}_vs_old_{ratio}_analysis.jpeg'\n",
    "        plt.savefig(plot_file_path, format='jpeg')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "# Load your datasets\n",
    "parent = '/home/jkim/research/peds_cxr/'\n",
    "\n",
    "file_paths = {\n",
    "    \"metrics_young_0\": parent + 'no_finding/results/age/analysis/csv/metrics_young_0.csv',\n",
    "    \"metrics_young_25\": parent + 'no_finding/results/age/analysis/csv/metrics_young_25.csv',\n",
    "    \"metrics_young_50\": parent + 'no_finding/results/age/analysis/csv/metrics_young_50.csv',\n",
    "    \"metrics_young_75\": parent + 'no_finding/results/age/analysis/csv/metrics_young_75.csv',\n",
    "    \"metrics_young_100\": parent + 'no_finding/results/age/analysis/csv/metrics_young_100.csv',\n",
    "    \"metrics_old_0\": parent + 'no_finding/results/age/analysis/csv/metrics_old_0.csv',\n",
    "    \"metrics_old_25\": parent + 'no_finding/results/age/analysis/csv/metrics_old_25.csv',\n",
    "    \"metrics_old_50\": parent + 'no_finding/results/age/analysis/csv/metrics_old_50.csv',\n",
    "    \"metrics_old_75\": parent + 'no_finding/results/age/analysis/csv/metrics_old_75.csv',\n",
    "    \"metrics_old_100\": parent + 'no_finding/results/age/analysis/csv/metrics_old_100.csv'\n",
    "}\n",
    "\n",
    "data = {name: pd.read_csv(path) for name, path in file_paths.items()}\n",
    "\n",
    "# Combine all datasets into a single DataFrame for easier plotting\n",
    "combined_data = pd.DataFrame()\n",
    "for name, df in data.items():\n",
    "    condition = name.split('_')[1]  # Either \"young\" or \"old\"\n",
    "    ratio = name.split('_')[2]  \n",
    "    df['Condition'] = 'Young' if condition == 'young' else 'Old'\n",
    "    df['Group'] = condition  \n",
    "    df['Ratio'] = ratio\n",
    "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "\n",
    "# Ensure correct data types for plotting\n",
    "combined_data['Ratio'] = pd.Categorical(combined_data['Ratio'], categories=['0', '25', '50', '75', '100'], ordered=True)\n",
    "\n",
    "# Load p-values dataset\n",
    "pvalues_file_path = parent + 'no_finding/results/age/analysis/csv/final_pvalues.csv'\n",
    "pvalues_data = pd.read_csv(pvalues_file_path)\n",
    "\n",
    "# Define labels and metric of interest\n",
    "labels = ['All Label', 'No Finding', 'Cardiomegaly', 'Consolidation', 'Infiltration', 'Mass/Nodule', 'Pneumonia']\n",
    "label_names = ['All Label', 'No Finding', 'Cardiomegaly', 'Consolidation', 'Infiltration', 'MassNodule', 'Pneumonia']\n",
    "metric = 'AUROC'\n",
    "neutral_palette = [\"#b5d1ae\", \"#80ae9a\", \"#568b87\", \"#1b485e\", \"#122740\"]\n",
    "\n",
    "# Define the merged comparison pairs\n",
    "comparison_pairs = [\n",
    "    ((\"young\", \"100\"), (\"old\", \"0\")),\n",
    "    ((\"young\", \"50\"), (\"old\", \"50\")),\n",
    "    ((\"young\", \"0\"), (\"old\", \"100\")),\n",
    "    ((\"young\", \"0\"), (\"young\", \"50\")),\n",
    "    ((\"young\", \"50\"), (\"young\", \"100\")),\n",
    "    ((\"young\", \"0\"), (\"young\", \"100\")),\n",
    "    ((\"old\", \"0\"), (\"old\", \"50\")),\n",
    "    ((\"old\", \"50\"), (\"old\", \"100\")),\n",
    "    ((\"old\", \"0\"), (\"old\", \"100\"))\n",
    "]\n",
    "\n",
    "for label, label_name in zip(labels, label_names):\n",
    "    # Filter data for the current label\n",
    "    filtered_data = combined_data[combined_data['Label'] == label]\n",
    "\n",
    "    # Extract relevant p-values for the comparison pairs\n",
    "    comparison_pvalues = []\n",
    "    for pair in comparison_pairs:\n",
    "        comparison = f\"{pair[0][0]}_{pair[0][1]} vs {pair[1][0]}_{pair[1][1]}\"\n",
    "        p_value_row = pvalues_data[(pvalues_data['Label'] == label) &\n",
    "                                   (pvalues_data['Metric'] == metric) &\n",
    "                                   (pvalues_data['Comparison'] == comparison)]\n",
    "        if not p_value_row.empty:\n",
    "            comparison_pvalues.append(p_value_row['p_value_corrected'].values[0])\n",
    "        else:\n",
    "            comparison_pvalues.append(None)\n",
    "            print('ERROR')\n",
    "\n",
    "    # Check if there is data for the current label\n",
    "    if not filtered_data.empty:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "\n",
    "        # Create the boxplot with two groups (Young and Old)\n",
    "        ax = sns.boxplot(\n",
    "            x='Group',\n",
    "            y=metric,\n",
    "            hue='Ratio',\n",
    "            data=filtered_data,\n",
    "            palette=neutral_palette + neutral_palette,\n",
    "            dodge=True\n",
    "        )\n",
    "\n",
    "        # Annotate all comparisons\n",
    "        annotator = Annotator(ax, comparison_pairs, data=filtered_data, x='Group', y=metric, hue='Ratio')\n",
    "        annotator.set_pvalues(comparison_pvalues)\n",
    "        annotator.configure(text_format=\"star\", loc=\"inside\", fontsize=12)\n",
    "        annotator.annotate()\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f'{label}', fontsize=20, fontweight='bold', pad=20)\n",
    "        plt.xlabel('Age Group', fontsize=18)\n",
    "        plt.ylabel('AUROC', fontsize=18)\n",
    "        # plt.legend(title='Training Set Schoolage Ratio (%)', loc='upper right')\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        plt.subplots_adjust(top=0.85)\n",
    "        ax.legend_.remove()\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.savefig(parent + f'no_finding/results/age/analysis/plot/age/{metric}_YoungvsOld_{label_name}.jpeg')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No data available for label: {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
